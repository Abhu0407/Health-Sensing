{"cells":[{"cell_type":"markdown","id":"03bfee7c","metadata":{"id":"03bfee7c"},"source":["\n","# Scenario 2 — Health Sensing\n","\n","This notebook implements the tasks described in *Scenario 2 - Health Sensing*:\n","- Visualizations for each participant (Nasal Airflow, Thoracic Movement, SpO₂) with event overlays\n","- Signal cleaning (bandpass filtering to retain breathing frequencies)\n","- Dataset creation: 30-second windows with 50% overlap and labeling from event annotations\n","- Model scaffolding for 1D CNN and Conv-LSTM and evaluation using Leave-One-Participant-Out (LOSO) CV\n","\n","\n","**Notes & assumptions**\n","\n","- Input data folder structure per participant should follow the assignment convention, e.g.:\n","\n","```\n","Data/AP01/\n","  nasal_airflow.txt\n","  thoracic_movement.txt\n","  spo2.txt\n","  flow_events.csv\n","  sleep_profile.csv  # optional\n","```\n","\n","- The notebook **does not** run long training loops automatically (training cells are provided and can be executed by the user). This keeps the notebook interactive and safe to run on modest hardware.\n","\n","- For full task text, refer to the provided PDF (Scenario 2 - Health Sensing)."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jaa07LWCg-UB","outputId":"e10865a1-d94a-46d9-b58b-2aa2f7122e22","executionInfo":{"status":"ok","timestamp":1763921620503,"user_tz":-330,"elapsed":24502,"user":{"displayName":"UI22CS02 ABHISHEK RATHORE","userId":"16276913289994994381"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"id":"jaa07LWCg-UB"},{"cell_type":"code","execution_count":2,"id":"7340c909","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7340c909","executionInfo":{"status":"ok","timestamp":1763921635357,"user_tz":-330,"elapsed":9881,"user":{"displayName":"UI22CS02 ABHISHEK RATHORE","userId":"16276913289994994381"}},"outputId":"95d45394-da97-42e4-aeb7-0e223671f8c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Libraries loaded.\n"]}],"source":["\n","# Standard imports\n","import os, json, math\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","from scipy.signal import butter, filtfilt, sosfiltfilt, sosfilt, sosfreqz\n","from datetime import timedelta\n","\n","# ML imports\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as optim\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n","\n","# Helper for reproducibility\n","RND = 42\n","np.random.seed(RND)\n","torch.manual_seed(RND)\n","\n","print('Libraries loaded.')\n"]},{"cell_type":"code","execution_count":7,"id":"c87a8024","metadata":{"id":"c87a8024","executionInfo":{"status":"ok","timestamp":1763922513060,"user_tz":-330,"elapsed":11,"user":{"displayName":"UI22CS02 ABHISHEK RATHORE","userId":"16276913289994994381"}}},"outputs":[],"source":["def load_signal_txt(path, time_col=0, value_col=1, time_format=None):\n","    \"\"\"\n","    Robust loader for breathing signals.\n","    Handles messy whitespace, extra columns, multiple delimiters, inconsistent rows.\n","    Always returns a pandas Series with a datetime index.\n","    \"\"\"\n","\n","    # STEP 1 — Load with extremely tolerant settings\n","    try:\n","        df = pd.read_csv(\n","            path,\n","            sep=r\"\\s+|,|;|\\t\",   # any whitespace, comma, semicolon or tab\n","            engine=\"python\",\n","            header=None,\n","            comment=\"#\",\n","            on_bad_lines=\"skip\"   # <--- prevents ParserError\n","        )\n","    except Exception as e:\n","        raise ValueError(f\"Could not read file {path}: {e}\")\n","\n","    # Remove fully empty columns\n","    df = df.dropna(axis=1, how='all')\n","\n","    # Ensure enough columns\n","    if df.shape[1] < max(time_col, value_col) + 1:\n","        raise ValueError(\n","            f\"File {path} does not contain enough columns. \"\n","            f\"Found {df.shape[1]}, but time_col={time_col}, value_col={value_col}\"\n","        )\n","\n","    # Extract time/value columns\n","    time_data = df.iloc[:, time_col]\n","    value_data = df.iloc[:, value_col]\n","\n","    # STEP 2 — Convert time column\n","    if pd.api.types.is_numeric_dtype(time_data):\n","        # treat as seconds offset\n","        timestamps = pd.to_timedelta(time_data, unit=\"s\")\n","        index = pd.Timestamp(\"2000-01-01\") + timestamps\n","    else:\n","        index = pd.to_datetime(time_data, format=time_format, errors=\"coerce\")\n","\n","    # STEP 3 — Convert values to float\n","    values = pd.to_numeric(value_data, errors=\"coerce\")\n","\n","    # Build Series\n","    s = pd.Series(values.values, index=index).dropna()\n","\n","    return s\n","\n","# Example usage:\n","# nasal = load_signal_txt('/content/drive/MyDrive/projects/Health_Sensing/Data/AP01/Flow-30-05-2024.txt', time_col=0, value_col=1)\n"]},{"cell_type":"code","execution_count":14,"id":"3ec7c0a8","metadata":{"id":"3ec7c0a8","executionInfo":{"status":"ok","timestamp":1763922976188,"user_tz":-330,"elapsed":45,"user":{"displayName":"UI22CS02 ABHISHEK RATHORE","userId":"16276913289994994381"}}},"outputs":[],"source":["\n","# ------------------------------------------------------------------------------------\n","# ✔ Robust Signal Loader (handles inconsistent columns, spaces, tabs, commas)\n","# ------------------------------------------------------------------------------------\n","def load_signal_txt(path, time_col=0, value_col=1, time_format=None):\n","    df = pd.read_csv(\n","        path,\n","        sep=r\"\\s+|,|;|\\t\",\n","        engine=\"python\",\n","        header=None,\n","        comment=\"#\",\n","        on_bad_lines=\"skip\"\n","    )\n","\n","    # Drop empty columns\n","    df = df.dropna(axis=1, how=\"all\")\n","\n","    # Extract required columns\n","    time_data = df.iloc[:, time_col]\n","    value_data = df.iloc[:, value_col]\n","\n","    # Convert time column\n","    if pd.api.types.is_numeric_dtype(time_data):\n","        index = pd.Timestamp(\"2000-01-01\") + pd.to_timedelta(time_data, unit=\"s\")\n","    else:\n","        index = pd.to_datetime(time_data, errors=\"coerce\")\n","\n","    # Convert values\n","    values = pd.to_numeric(value_data, errors=\"coerce\")\n","\n","    # Build series\n","    s = pd.Series(values.values, index=index).dropna()\n","    return s\n","\n","\n","# ------------------------------------------------------------------------------------\n","# ✔ Robust Event Loader (handles spaces, tabs, extra columns)\n","# ------------------------------------------------------------------------------------\n","def load_events(path):\n","    df = pd.read_csv(\n","        path,\n","        sep=r\"\\s+|,|;|\\t\",\n","        engine=\"python\",\n","        header=None,\n","        on_bad_lines=\"skip\",\n","        comment=\"#\"\n","    )\n","\n","    # Drop empty columns\n","    df = df.dropna(axis=1, how=\"all\")\n","\n","    # Event file usually has 3 columns → start, end, type\n","    if df.shape[1] < 3:\n","        raise ValueError(f\"Event file {path} must have at least 3 columns.\")\n","\n","    df.columns = [\"start_time\", \"end_time\", \"event_type\"]\n","\n","    df[\"start_time\"] = pd.to_datetime(df[\"start_time\"], errors=\"coerce\")\n","    df[\"end_time\"] = pd.to_datetime(df[\"end_time\"], errors=\"coerce\")\n","\n","    return df.dropna()\n","\n","\n","# ------------------------------------------------------------------------------------\n","# ✔ Fixed Visualization Function\n","# ------------------------------------------------------------------------------------\n","def visualize_participant(part_dir, out_pdf_path=None, show=False):\n","\n","    p = Path(part_dir)\n","\n","    nasal_path = p / 'Flow-30-05-2024.txt'\n","    thor_path  = p / 'Thorac-30-05-2024.txt'\n","    spo2_path  = p / 'SPO2-30-05-2024.txt'\n","    events_path = p / 'FlowEvents-30-05-2024.txt'\n","\n","    # Load signals robustly\n","    nasal = load_signal_txt(nasal_path)\n","    thor = load_signal_txt(thor_path)\n","    spo2 = load_signal_txt(spo2_path)\n","\n","    # Load events robustly\n","    events = load_events(events_path) if events_path.exists() else None\n","\n","    # Align thor + spo2 to nasal timestamps\n","    base_idx = nasal.index\n","    thor_r = thor.reindex(base_idx, method=\"nearest\")\n","    spo2_r = spo2.reindex(base_idx, method=\"nearest\")\n","\n","    # Output PDF\n","    if out_pdf_path is None:\n","        out_pdf_path = p / f\"{p.name}_visualization.pdf\"\n","\n","    with PdfPages(out_pdf_path) as pdf:\n","        plt.figure(figsize=(16, 10))\n","\n","        ax1 = plt.subplot(3, 1, 1)\n","        ax1.plot(base_idx, nasal, label=\"Nasal Airflow\")\n","        ax1.set_ylabel(\"Airflow\")\n","        if events is not None:\n","            for _, r in events.iterrows():\n","                ax1.axvspan(r.start_time, r.end_time, alpha=0.3)\n","        ax1.legend()\n","\n","        ax2 = plt.subplot(3, 1, 2, sharex=ax1)\n","        ax2.plot(base_idx, thor_r, label=\"Thoracic Movement\")\n","        ax2.set_ylabel(\"Thorax\")\n","        if events is not None:\n","            for _, r in events.iterrows():\n","                ax2.axvspan(r.start_time, r.end_time, alpha=0.3)\n","\n","        ax3 = plt.subplot(3, 1, 3, sharex=ax1)\n","        ax3.plot(base_idx, spo2_r, label=\"SpO₂\")\n","        ax3.set_ylabel(\"SpO₂ (%)\")\n","        ax3.set_xlabel(\"Time\")\n","        if events is not None:\n","            for _, r in events.iterrows():\n","                ax3.axvspan(r.start_time, r.end_time, alpha=0.3)\n","\n","        plt.tight_layout()\n","        pdf.savefig()\n","        if show:\n","            plt.show()\n","        plt.close()\n","\n","    print(f\"Visualization saved to: {out_pdf_path}\")\n","    return out_pdf_path\n","\n","\n","# Example:\n","# visualize_participant('/content/drive/MyDrive/projects/Health_Sensing/Data/AP01')\n"]},{"cell_type":"code","execution_count":18,"id":"745bd423","metadata":{"id":"745bd423","executionInfo":{"status":"ok","timestamp":1763923552831,"user_tz":-330,"elapsed":15,"user":{"displayName":"UI22CS02 ABHISHEK RATHORE","userId":"16276913289994994381"}}},"outputs":[],"source":["\n","def bandpass_butter_lowlevel(signal, fs, lowcut=0.08, highcut=0.6, order=4):\n","    \"\"\"Design and apply a Butterworth bandpass filter using second-order sections (stable).\n","    Default lowcut/highcut chosen to retain typical breathing frequencies (0.08-0.6 Hz).\n","    fs: sampling frequency in Hz\n","    Returns filtered signal (pandas Series if input was Series)\n","    \"\"\"\n","    nyq = 0.5 * fs\n","    low = lowcut / nyq\n","    high = highcut / nyq\n","    sos = butter(order, [low, high], analog=False, btype='band', output='sos')\n","    x = signal.values if isinstance(signal, pd.Series) else np.asarray(signal)\n","    y = sosfiltfilt(sos, x)\n","    if isinstance(signal, pd.Series):\n","        return pd.Series(y, index=signal.index)\n","    else:\n","        return y\n","\n","# Example usage:\n","# fs = 32\n","# nasal_f = bandpass_butter_lowlevel(nasal, fs)\n"]},{"cell_type":"code","execution_count":19,"id":"7c4f4c76","metadata":{"id":"7c4f4c76","executionInfo":{"status":"ok","timestamp":1763923561686,"user_tz":-330,"elapsed":17,"user":{"displayName":"UI22CS02 ABHISHEK RATHORE","userId":"16276913289994994381"}}},"outputs":[],"source":["\n","def sliding_windows_labeling(nasal, thor, spo2, events_df, window_s=30, overlap=0.5, fs=32):\n","    \"\"\"Split signals into windows of window_s seconds with overlap and assign labels.\n","    events_df is expected to contain columns ['start_time','end_time','event_type'] with datetime type.\n","    Returns a DataFrame with columns: ['start', 'end', 'label', 'nasal', 'thor', 'spo2']\n","    where nasal/thor/spo2 are numpy arrays for the window.\n","    \"\"\"\n","    step = int(window_s * (1 - overlap) * fs)\n","    win_len = int(window_s * fs)\n","\n","    # Use nasal index as time grid and convert to numeric seconds relative to start\n","    t0 = nasal.index[0]\n","    total_samples = len(nasal)\n","\n","    windows = []\n","    for start_idx in range(0, total_samples - win_len + 1, step):\n","        end_idx = start_idx + win_len\n","        start_time = nasal.index[start_idx]\n","        end_time = nasal.index[end_idx-1]\n","\n","        # Determine overlap with events\n","        label = 'Normal'\n","        if events_df is not None and len(events_df)>0:\n","            # compute overlap duration for each event\n","            overlaps = []\n","            for _, ev in events_df.iterrows():\n","                s = ev['start_time']\n","                e = ev['end_time']\n","                # intersection\n","                inter_start = max(s, start_time)\n","                inter_end = min(e, end_time)\n","                overlap_seconds = max((inter_end - inter_start).total_seconds(), 0)\n","                overlaps.append((overlap_seconds, ev.get('event_type','Event')))\n","            # if any event overlaps more than 50% of window, assign that label\n","            max_overlap, ev_type = max(overlaps, key=lambda x: x[0]) if overlaps else (0, None)\n","            if max_overlap >= 0.5*window_s:\n","                label = ev_type\n","\n","        nasal_win = nasal.iloc[start_idx:end_idx].values\n","        thor_win = thor.iloc[start_idx:end_idx].values if len(thor)>=end_idx else np.pad(thor.iloc[start_idx:end_idx].values, (0, max(0, win_len - len(thor.iloc[start_idx:end_idx].values))))\n","        spo2_win = spo2.iloc[start_idx:end_idx*int(4/32) if False else start_idx:end_idx].values if False else spo2.reindex(nasal.index).iloc[start_idx:end_idx].values\n","\n","        windows.append({'start':start_time, 'end':end_time, 'label':label,\n","                        'nasal':nasal_win, 'thor':thor_win, 'spo2':spo2_win})\n","    dfw = pd.DataFrame(windows)\n","    return dfw\n","\n","# Note: the function above assumes signals were resampled/reindexed to the same index (use reindex as shown in visualize_participant)\n"]},{"cell_type":"code","execution_count":20,"id":"8d6b4ab8","metadata":{"id":"8d6b4ab8","executionInfo":{"status":"ok","timestamp":1763923565613,"user_tz":-330,"elapsed":3,"user":{"displayName":"UI22CS02 ABHISHEK RATHORE","userId":"16276913289994994381"}}},"outputs":[],"source":["\n","def save_dataset_windows(df_windows, out_path):\n","    \"\"\"Save the windows DataFrame to a parquet file. We convert the numpy arrays to lists\n","    to make them serializable in parquet.\n","    \"\"\"\n","    df = df_windows.copy()\n","    df['nasal'] = df['nasal'].apply(lambda x: x.tolist())\n","    df['thor'] = df['thor'].apply(lambda x: x.tolist())\n","    df['spo2'] = df['spo2'].apply(lambda x: x.tolist())\n","    out_path = Path(out_path)\n","    out_path.parent.mkdir(parents=True, exist_ok=True)\n","    df.to_parquet(out_path, index=False)\n","    print(f\"Saved dataset to {out_path}\")\n","\n","# Example usage:\n","# save_dataset_windows(dfw, 'Dataset/AP01_windows.parquet')\n"]},{"cell_type":"code","execution_count":21,"id":"193bd17e","metadata":{"id":"193bd17e","executionInfo":{"status":"ok","timestamp":1763923569263,"user_tz":-330,"elapsed":4,"user":{"displayName":"UI22CS02 ABHISHEK RATHORE","userId":"16276913289994994381"}}},"outputs":[],"source":["\n","class BreathingWindowDataset(Dataset):\n","    def __init__(self, df_windows, transform=None):\n","        self.df = df_windows.reset_index(drop=True)\n","        self.transform = transform\n","        self.label_map = {'Normal':0, 'Hypopnea':1, 'Obstructive Apnea':2}\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.loc[idx]\n","        x_nasal = np.array(row['nasal'], dtype=np.float32)\n","        x_thor = np.array(row['thor'], dtype=np.float32)\n","        # Stack channels: nasal and thor -> shape (channels, timesteps)\n","        x = np.stack([x_nasal, x_thor], axis=0)\n","        y = self.label_map.get(row['label'], 0)\n","        return x, y\n"]},{"cell_type":"code","execution_count":22,"id":"c684b75f","metadata":{"id":"c684b75f","executionInfo":{"status":"ok","timestamp":1763923578622,"user_tz":-330,"elapsed":5,"user":{"displayName":"UI22CS02 ABHISHEK RATHORE","userId":"16276913289994994381"}}},"outputs":[],"source":["\n","class Simple1DCNN(nn.Module):\n","    def __init__(self, in_channels=2, n_classes=3):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Conv1d(in_channels, 16, kernel_size=7, padding=3),\n","            nn.ReLU(),\n","            nn.MaxPool1d(2),\n","            nn.Conv1d(16, 32, kernel_size=5, padding=2),\n","            nn.ReLU(),\n","            nn.MaxPool1d(2),\n","            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.AdaptiveAvgPool1d(1),\n","            nn.Flatten(),\n","            nn.Linear(64, n_classes)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","# Example instantiation\n","model = Simple1DCNN()\n"]},{"cell_type":"code","execution_count":23,"id":"efafef47","metadata":{"id":"efafef47","executionInfo":{"status":"ok","timestamp":1763923584291,"user_tz":-330,"elapsed":21,"user":{"displayName":"UI22CS02 ABHISHEK RATHORE","userId":"16276913289994994381"}}},"outputs":[],"source":["\n","class ConvLSTMBlock(nn.Module):\n","    def __init__(self, in_channels=2, hidden_dim=64, n_classes=3):\n","        super().__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv1d(in_channels, 32, kernel_size=7, padding=3),\n","            nn.ReLU(),\n","            nn.MaxPool1d(2)\n","        )\n","        # LSTM expects (seq_len, batch, features), so we transpose accordingly\n","        self.lstm = nn.LSTM(input_size=32, hidden_size=hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, n_classes)\n","\n","    def forward(self, x):\n","        # x: (batch, channels, timesteps)\n","        c = self.conv(x)  # (batch, filters, t')\n","        c = c.permute(0, 2, 1)  # (batch, t', features)\n","        out, _ = self.lstm(c)\n","        out = out[:, -1, :]\n","        return self.fc(out)\n","\n","# model = ConvLSTMBlock()\n"]},{"cell_type":"code","execution_count":24,"id":"d8e2d3b5","metadata":{"id":"d8e2d3b5","executionInfo":{"status":"ok","timestamp":1763923586755,"user_tz":-330,"elapsed":38,"user":{"displayName":"UI22CS02 ABHISHEK RATHORE","userId":"16276913289994994381"}}},"outputs":[],"source":["\n","def loso_train_and_evaluate(all_windows_df, participant_column='participant', epochs=10, batch_size=32, device='cpu'):\n","    \"\"\"Perform Leave-One-Subject-Out CV. all_windows_df must contain a column with participant ids.\n","    This function is a skeleton: it trains a model for each fold and collects metrics.\n","    \"\"\"\n","    participants = all_windows_df[participant_column].unique()\n","    results = {}\n","    for test_p in participants:\n","        print('Fold - test participant:', test_p)\n","        train_df = all_windows_df[all_windows_df[participant_column]!=test_p]\n","        test_df = all_windows_df[all_windows_df[participant_column]==test_p]\n","\n","        train_ds = BreathingWindowDataset(train_df)\n","        test_ds = BreathingWindowDataset(test_df)\n","        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","        test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n","\n","        model = Simple1DCNN()\n","        model.to(device)\n","        criterion = nn.CrossEntropyLoss()\n","        opt = optim.Adam(model.parameters(), lr=1e-3)\n","\n","        # training loop (small number of epochs here)\n","        for ep in range(epochs):\n","            model.train()\n","            for xb, yb in train_loader:\n","                xb = xb.to(device)\n","                yb = yb.to(device)\n","                out = model(xb)\n","                loss = criterion(out, yb)\n","                opt.zero_grad()\n","                loss.backward()\n","                opt.step()\n","\n","        # evaluation\n","        model.eval()\n","        y_true = []\n","        y_pred = []\n","        with torch.no_grad():\n","            for xb, yb in test_loader:\n","                xb = xb.to(device)\n","                out = model(xb)\n","                preds = out.argmax(dim=1).cpu().numpy()\n","                y_pred.extend(preds.tolist())\n","                y_true.extend(yb.numpy().tolist())\n","\n","        cm = confusion_matrix(y_true, y_pred)\n","        acc = accuracy_score(y_true, y_pred)\n","        prec = precision_score(y_true, y_pred, average=None, zero_division=0)\n","        rec = recall_score(y_true, y_pred, average=None, zero_division=0)\n","\n","        results[test_p] = {'confusion_matrix': cm, 'accuracy': acc, 'precision': prec.tolist(), 'recall': rec.tolist()}\n","\n","    return results\n","\n","# NOTE: Running LOSO with actual training can take time. Execute interactively with smaller epochs while testing.\n"]},{"cell_type":"code","source":["# Assuming your data is in '/content/drive/MyDrive/projects/Health_Sensing/Data/'\n","# Adjust this path if your data is located elsewhere\n","data_root = Path('/content/drive/MyDrive/projects/Health_Sensing/Data/')\n","\n","all_participant_windows = []\n","fs = 32 # Sampling frequency, based on examples in bandpass_butter_lowlevel\n","\n","# Loop through each participant directory to load, preprocess, and window their data\n","for participant_dir in data_root.iterdir():\n","    # Check if it's a directory and starts with 'AP' (e.g., AP01, AP02)\n","    if participant_dir.is_dir() and participant_dir.name.startswith('AP'):\n","        print(f\"Processing participant: {participant_dir.name}\")\n","\n","        # Define paths to signal and event files based on the structure in visualize_participant\n","        nasal_path = participant_dir / 'Flow-30-05-2024.txt'\n","        thor_path  = participant_dir / 'Thorac-30-05-2024.txt'\n","        spo2_path  = participant_dir / 'SPO2-30-05-2024.txt'\n","        events_path = participant_dir / 'FlowEvents-30-05-2024.txt'\n","\n","        # Skip if essential signal files are missing\n","        if not (nasal_path.exists() and thor_path.exists() and spo2_path.exists()):\n","            print(f\"Skipping {participant_dir.name}: Missing essential signal files. Ensure paths are correct.\")\n","            continue\n","\n","        try:\n","            # 1. Load signals using the robust loader\n","            nasal = load_signal_txt(nasal_path, time_col=0, value_col=1)\n","            thor = load_signal_txt(thor_path, time_col=0, value_col=1)\n","            spo2 = load_signal_txt(spo2_path, time_col=0, value_col=1)\n","\n","            # 2. Load events (optional, handles if file doesn't exist)\n","            events = load_events(events_path) if events_path.exists() else None\n","\n","            # 3. Align thoracic and SpO2 signals to the nasal airflow's timestamp index\n","            # This ensures all signals are on the same time grid for windowing\n","            base_idx = nasal.index\n","            thor_aligned = thor.reindex(base_idx, method=\"nearest\")\n","            spo2_aligned = spo2.reindex(base_idx, method=\"nearest\")\n","\n","            # 4. Apply bandpass filter to breathing-related signals (nasal and thoracic)\n","            # SpO2 is typically not bandpass filtered for breathing rhythms\n","            nasal_filtered = bandpass_butter_lowlevel(nasal, fs=fs)\n","            thor_filtered = bandpass_butter_lowlevel(thor_aligned, fs=fs)\n","            spo2_final = spo2_aligned # Use the aligned SpO2 directly\n","\n","            # 5. Create 30-second windows with 50% overlap\n","            participant_windows_df = sliding_windows_labeling(\n","                nasal=nasal_filtered,\n","                thor=thor_filtered,\n","                spo2=spo2_final,\n","                events_df=events,\n","                window_s=30,\n","                overlap=0.5,\n","                fs=fs\n","            )\n","\n","            # Add a 'participant' column to identify the source of each window\n","            participant_windows_df['participant'] = participant_dir.name\n","            all_participant_windows.append(participant_windows_df)\n","\n","        except Exception as e:\n","            print(f\"Error processing participant {participant_dir.name}: {e}\")\n","            continue\n","\n","# Check if any data was processed\n","if not all_participant_windows:\n","    print(\"No participant data was processed. Please check your data directory structure and file names.\")\n","else:\n","    # 6. Concatenate all participant windows into a single DataFrame\n","    combined_windows_df = pd.concat(all_participant_windows, ignore_index=True)\n","    print(f\"\\nTotal windows generated across all participants: {len(combined_windows_df)}\")\n","    print(f\"Combined DataFrame head:\\n{combined_windows_df.head()}\")\n","\n","    # 7. Evaluate the model using Leave-One-Subject-Out (LOSO) cross-validation\n","    print(\"\\nStarting Leave-One-Subject-Out (LOSO) cross-validation...\")\n","\n","    # Determine device for training (GPU if available, otherwise CPU)\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    print(f\"Using device: {device}\")\n","\n","    # Run LOSO evaluation. Note: 'epochs' is set low for quick demonstration.\n","    # For robust training and evaluation, you would typically increase the number of epochs.\n","    loso_results = loso_train_and_evaluate(\n","        combined_windows_df,\n","        participant_column='participant',\n","        epochs=5, # Consider increasing this for actual training (e.g., 50-100)\n","        batch_size=32,\n","        device=device\n","    )\n","\n","    print(\"\\nLOSO Evaluation Results:\")\n","    for participant, metrics in loso_results.items():\n","        print(f\"-- Participant {participant} (test set) --\")\n","        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n","        # Precision and Recall are provided per class (Normal, Hypopnea, Obstructive Apnea)\n","        # Ensure your dataset contains these labels if you expect non-zero scores.\n","        print(f\"  Precision (Normal, Hypopnea, Obstructive Apnea): {metrics['precision']}\")\n","        print(f\"  Recall (Normal, Hypopnea, Obstructive Apnea): {metrics['recall']}\")\n","        print(f\"  Confusion Matrix:\\n{metrics['confusion_matrix']}\")\n","        print(\"-\" * 40)\n","\n","    # Calculate and print average accuracy across all folds\n","    if loso_results:\n","        avg_accuracy = np.mean([res['accuracy'] for res in loso_results.values()])\n","        print(f\"\\nAverage Accuracy across all LOSO folds: {avg_accuracy:.4f}\")\n","    else:\n","        print(\"No results to aggregate.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wxCgL8XKCkJy","executionInfo":{"status":"ok","timestamp":1763923714891,"user_tz":-330,"elapsed":25960,"user":{"displayName":"UI22CS02 ABHISHEK RATHORE","userId":"16276913289994994381"}},"outputId":"8b762756-f597-4ac1-dfdf-1a3f8933f69a"},"id":"wxCgL8XKCkJy","execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing participant: AP04\n","Skipping AP04: Missing essential signal files. Ensure paths are correct.\n","Processing participant: AP02\n","Skipping AP02: Missing essential signal files. Ensure paths are correct.\n","Processing participant: AP05\n","Skipping AP05: Missing essential signal files. Ensure paths are correct.\n","Processing participant: AP03\n","Skipping AP03: Missing essential signal files. Ensure paths are correct.\n","Processing participant: AP01\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3440653294.py:25: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  index = pd.to_datetime(time_data, errors=\"coerce\")\n","/tmp/ipython-input-3440653294.py:25: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  index = pd.to_datetime(time_data, errors=\"coerce\")\n"]},{"output_type":"stream","name":"stdout","text":["Error processing participant AP01: The length of the input vector x must be greater than padlen, which is 27.\n","No participant data was processed. Please check your data directory structure and file names.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3440653294.py:25: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  index = pd.to_datetime(time_data, errors=\"coerce\")\n","/tmp/ipython-input-3440653294.py:57: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  df[\"start_time\"] = pd.to_datetime(df[\"start_time\"], errors=\"coerce\")\n","/tmp/ipython-input-3440653294.py:58: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  df[\"end_time\"] = pd.to_datetime(df[\"end_time\"], errors=\"coerce\")\n"]}]},{"cell_type":"markdown","id":"705e3fbc","metadata":{"id":"705e3fbc"},"source":["\n","## How to use this notebook\n","\n","1. Place participant folders under `Data/` following the naming convention (AP01, AP02, ...).\n","2. Run the *Imports* cell.\n","3. Use `visualize_participant('Data/AP01')` to generate PDFs for individual participants.\n","4. Use the filtering functions to preprocess signals before windowing (e.g., `bandpass_butter_lowlevel`).\n","5. Create windows with `sliding_windows_labeling` (make sure to reindex signals to same sampling grid first).\n","6. Save datasets with `save_dataset_windows` (parquet recommended).\n","7. Train models using `loso_train_and_evaluate` — adjust `epochs`, `device` and `batch_size` as needed.\n","\n","---\n","\n","**Recommendation for saving dataset format:** Parquet is compact, fast to read, and supports columns with lists. It integrates well with pandas and PyArrow.\n","\n","Good luck — run cells interactively and adapt paths & column names to your exact data files.\n"]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}